---
title: "强化学习0511"
description: "Shuaiyi Huang 等人的 [TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations](http://arxiv.org/pdf/2505.06079v1 ) 提出了 TREND 框架，通过三教学策略和少量专家示范解决偏好反馈中的噪声问题。TREND 同时训练三个奖励模型，通过互相教授小损失偏好对来更新参数。实验表明，TREND 在高达40%噪声水平下仍能实现90%的成功率，展示了其在处理噪声偏好反馈中的鲁棒性。该成果于2025年发表于 ICRA。"
date: 2025-12-30
lang: zh
slug: "强化学习0511"
tags: []
featured: false
draft: false
---

Kwan-Yee Lin 等人的 [Let Humanoids Hike! Integrative Skill Development on Complex Trails](http://arxiv.org/pdf/2505.06218v1 ) 提出了一种训练人形机器人在复杂地形上自主徒步的学习框架 LEGO-H。该框架通过**时序视觉Transformer**和**分层强化学习**的结合，实现了视觉感知、决策和运动执行的综合技能开发。LEGO-H 能够处理多样化的物理和环境挑战，无需依赖预定义的运动模式。实验表明，LEGO-H 在模拟复杂地形和不同机器人形态下表现出强大的适应性和鲁棒性。该成果于2025年发表于 CVPR。

Tim Schneider 等人的 [Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based Approach](http://arxiv.org/pdf/2505.06182v1 ) 提出了 TAP 框架，通过强化学习和 Transformer 架构解决部分可观测环境中的主动触觉感知问题。TAP 结合了 Soft Actor-Critic (SAC) 和 CrossQ 算法，实现了感知模块和决策策略的联合训练。实验表明，TAP 在触觉 MNIST 数字识别和触觉姿态估计任务中表现出色，展示了其在机器人主动触觉感知中的潜力。

Jiacheng Lin 等人的 [Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning](http://arxiv.org/pdf/2503.24289v2 ) 提出了 Rec-R1 框架，通过强化学习将大语言模型与推荐系统结合。Rec-R1 直接优化大语言模型的生成，利用固定推荐模型的反馈进行闭环优化，避免了数据蒸馏的高成本。实验表明，Rec-R1 在商品搜索和序列推荐任务中优于基于提示和监督微调的方法，并且保持了语言模型的通用能力。

Haokun Yu 等人的 [Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled Systems via Particle Filter Reinforcement Learning](http://arxiv.org/pdf/2505.06122v1 ) 提出了一种基于粒子滤波强化学习的交互感知隐私保护数据共享方法。该方法通过最小化互信息和失真数据对控制性能的影响，实现了敏感参数的隐私保护。在混合自动驾驶车队场景中，该方法有效保护了人类驾驶车辆的敏感参数，同时对燃料效率的影响可忽略。该成果于2025年发表于 L4DC 会议。

Shuaiyi Huang 等人的 [TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations](http://arxiv.org/pdf/2505.06079v1 ) 提出了 TREND 框架，通过三教学策略和少量专家示范解决偏好反馈中的噪声问题。TREND 同时训练三个奖励模型，通过互相教授小损失偏好对来更新参数。实验表明，TREND 在高达40%噪声水平下仍能实现90%的成功率，展示了其在处理噪声偏好反馈中的鲁棒性。该成果于2025年发表于 ICRA。

Rustem Islamov 等人的 [Safe-EF: Error Feedback for Nonsmooth Constrained Optimization](http://arxiv.org/pdf/2505.06053v1 ) 提出了一种新的算法 Safe-EF，用于处理非光滑约束优化问题。该算法通过**误差反馈（EF）**机制解决了通信压缩带来的性能下降问题，并在强化学习环境中验证了其有效性，特别是在分布式人形机器人训练中，确保了安全性并降低了通信复杂度。

Yi-Fan Zhang 等人的 [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](http://arxiv.org/pdf/2505.02835v2 ) 提出了一种基于强化学习的多模态奖励模型（MRM）训练方法。通过改进现有的强化学习算法，提出了**StableReinforce**算法，显著提高了多模态奖励模型的性能，在基准测试中分别提升了 8.4% 和 14.3%。

Jianpeng Qi 等人的 [Efficient Information Updates in Compute-First Networking via Reinforcement Learning with Joint AoI and VoI](http://arxiv.org/pdf/2505.06025v1 ) 提出了一种基于强化学习的信息更新策略，通过联合**信息时效性（AoI）**和**信息价值（VoI）**，在计算优先网络中实现了高效的信息更新，减少了 90% 以上的更新频率。

Stavros Orfanoudakis 等人的 [GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments](http://arxiv.org/pdf/2502.01778v2 ) 提出了一种结合图神经网络（GNN）和决策变换器（DT）的架构 GNN-DT，用于动态环境中的优化问题。该模型在电动汽车充电优化问题中表现出色，显著提高了样本效率并具有强大的泛化能力。

Bart van Laatum 等人的 [GreenLight-Gym: Reinforcement learning benchmark environment for control of greenhouse production systems](http://arxiv.org/pdf/2410.05336v2 ) 提出了一个开源的强化学习基准环境 GreenLight-Gym，用于温室作物生产控制。该环境基于 GreenLight 模型，仿真速度提高了 17 倍，为温室控制研究提供了标准化基准。

Dan Qiao 等人的 [Offline Multi-agent Reinforcement Learning via Score Decomposition](http://arxiv.org/pdf/2505.05968v1 ) 提出了一种针对离线多智能体强化学习（MARL）的新框架。该框架通过**扩散生成模型**捕捉复杂的行为策略，并引入**序列评分函数分解机制**来正则化个体策略，从而实现分散执行。实验表明，该方法在多个标准离线MARL基准测试中取得了**26.3%**的性能提升，为多智能体系统的协调和均衡选择提供了新的见解。

Uyoata E. Uyoata 等人的 [Learning Power Control Protocol for In-Factory 6G Subnetworks](http://arxiv.org/pdf/2505.05967v1 ) 提出了一种多智能体强化学习（MARL）框架，用于在工厂内6G子网络中实现自主学习和功率控制协议。通过将问题建模为**部分可观测马尔可夫决策过程（POMDP）**，并结合**多智能体近端策略优化（MAPPO）**，该方法在减少信令开销的同时保持了较高的性能。该成果将于2025年发表于 IEEE EuCNC & 6G Summit。

Xiyu Wang 等人的 [Multi-User Beamforming with Deep Reinforcement Learning in Sensing-Aided Communication](http://arxiv.org/pdf/2505.05956v1 ) 提出了一种基于深度强化学习（DRL）的多用户波束成形优化方法。该方法通过**多波束分配策略**和**DRL辅助优化**，显著提高了毫米波通信中的吞吐量，且无需用户反馈。该成果将于2025年发表于 IEEE EuCNC & 6G Summit。

Florinel-Alin Croitoru 等人的 [Curriculum Direct Preference Optimization for Diffusion and Consistency Models](http://arxiv.org/pdf/2405.13637v6 ) 提出了一种基于课程学习的直接偏好优化（DPO）方法，用于文本到图像生成任务。该方法通过**逐步增加难度**的样本对训练生成模型，在文本对齐、美学和人类偏好方面优于现有方法。该成果将于2025年发表于 CVPR。

Gijeong Kim 等人的 [A Learning Framework for Diverse Legged Robot Locomotion Using Barrier-Based Style Rewards](http://arxiv.org/pdf/2409.15780v4 ) 提出了一种基于**屏障函数风格奖励**的无模型强化学习框架，用于实现多样化的腿式机器人运动模式。实验表明，该框架支持四足、三足和双足运动，并能在复杂环境中高效运行。该成果将于2025年发表于 IEEE International Conference on Robotics and Automation (ICRA)。

Xinyu Liang 等人的 [Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency](http://arxiv.org/pdf/2505.05796v1 ) 提出了一种**人机交互（HITL）**的人工智能框架，用于优化HVAC系统的性能和能源效率。该方法通过**实时用户反馈**和**强化学习**动态调整系统运行，在降低能源成本的同时提升用户舒适度。该成果将于2025年发表于 ACM e-Energy。

Songchen Fu 等人的 [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](http://arxiv.org/pdf/2505.03586v2 ) 提出了一种多智能体强化学习（MARL）框架，用于解决多智能体系统中的观测延迟问题。该研究首先扩展了标准的Dec-POMDP，提出了分散式随机个体延迟部分可观测马尔可夫决策过程（DSID-POMDP），并在此基础上提出了Rainbow Delay Compensation（RDC）框架。实验表明，**RDC框架能够显著缓解延迟带来的性能下降，在某些延迟场景下甚至达到了无延迟的性能水平**。该工作为多智能体延迟观测问题提供了新的视角和解决方案。

Jongchan Park 等人的 [Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning](http://arxiv.org/pdf/2505.05701v1 ) 提出了一种简单但有效的预训练方法，用于初始化Q网络的特征，以提高离线强化学习的数据效率。该研究引入了一种共享Q网络结构，能够同时预测下一个状态和Q值，并通过监督回归任务进行预训练。实验表明，**该方法在D4RL、Robomimic和V-D4RL等基准测试中显著提升了现有离线强化学习方法的性能，甚至在仅使用10%数据集的情况下也能超越标准算法**。

Renhao Wang 等人的 [Prioritized Generative Replay](http://arxiv.org/pdf/2410.18082v2 ) 提出了一种优先级的生成式记忆方法，利用生成模型来捕捉在线经验。该方法通过**生成模型的泛化能力对过去经验进行密集化，并通过“相关函数”引导生成更相关的样本**。实验表明，该方法在状态和像素域中均能提高性能和样本效率，并展示了其在高更新数据比情况下的潜力。

Zixuan Wu 等人的 [Diffusion-Reinforcement Learning Hierarchical Motion Planning in Multi-agent Adversarial Games](http://arxiv.org/pdf/2403.10794v2 ) 提出了一种分层架构，结合了高层扩散模型和低层强化学习策略，用于多智能体对抗游戏中的运动规划。实验表明，**该方法在不同领域和不同可观测性条件下均优于基线方法，在检测率和目标达成率上分别提升了77.18%和47.38%**，并提高了策略的可解释性和灵活性。

Noorbakhsh Amiri Golilarz 等人的 [Learning Algorithms Made Simple](http://arxiv.org/pdf/2410.09186v2 ) 对学习算法及其在不同应用中的重要性进行了综述，涵盖了人工智能、机器学习、深度学习和混合模型的主要概念。文章还讨论了**学习算法与大语言模型（LLM）的集成**，以及下一代学习算法的发展方向，为学习算法的现状、应用和未来方向提供了简要概述。

Eric Squires 等人的 [Barrier Function Overrides For Non-Convex Fixed Wing Flight Control and Self-Driving Cars](http://arxiv.org/pdf/2505.05548v1 ) 提出了一种用于非凸系统的屏障函数覆盖方法，以解决强化学习（RL）在安全关键系统中的探索问题。通过开发离散时间下的屏障函数，该方法能够在固定翼飞机和自动驾驶汽车的车道合并与自适应巡航控制等非凸系统中实现近似最优的覆盖，**即使不求解最优覆盖，性能仍能与基线RL方法相当**。该成果已提交IEEE进行可能的出版。

Jie Liu 等人的 [Flow-GRPO: Training Flow Matching Models via Online RL](http://arxiv.org/pdf/2505.05470v1 ) 提出了Flow-GRPO，这是第一个将在线强化学习（RL）与流匹配模型结合的方法。通过将确定性ODE转换为等效的SDE，以及减少训练去噪步骤的策略，**Flow-GRPO在文本生成任务中显著提高了生成图像的准确性和人类偏好对齐**，且未出现奖励黑客行为。

Pouria Behnoudfar 等人的 [RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles](http://arxiv.org/pdf/2505.05452v1 ) 提出了一种基于强化学习的数据同化方法RL-DAUNCE。该方法通过将代理设计为集合成员，**强调不确定性量化并支持物理约束的强制执行**，在Madden-Julian Oscillation的模拟中表现优于标准的集合卡尔曼滤波，且计算效率更高。

Yanda Chen 等人的 [Reasoning Models Don't Always Say What They Think](http://arxiv.org/pdf/2505.05410v1 ) 研究了推理模型的思维链（CoT）是否忠实反映其实际推理过程。实验表明，**CoT监控在训练和评估中能发现一些不期望的行为，但无法完全排除这些行为**，尤其是在不需要CoT推理的场景中。

Hanwen Du 等人的 [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](http://arxiv.org/pdf/2410.09580v3 ) 提出了基于蒙特卡罗树搜索（MCTS）的对话推荐系统框架SAPIENT。通过结合对话代理和对话规划器，**SAPIENT在四个基准数据集上表现优于现有方法**，并在训练效率和性能之间取得了平衡。该成果已被NAACL 2025主会议接收。

Zechu Li 等人的 [Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation](http://arxiv.org/pdf/2505.05287v1 ) 提出了 SYMDEX，一个利用机器人双边对称性作为归纳偏置的强化学习框架，用于双手灵巧操作。**该框架通过将复杂的双手操作任务分解为单手的子任务，并训练专门的策略，利用等变神经网络使左右手经验共享。** 最终，子任务策略被蒸馏为一个全局的双手灵巧策略，能够独立于手-任务分配执行任务。实验表明，SYMDEX 在复杂任务中显著优于基线方法，并成功应用于真实世界和四臂操作场景。

Andreas Kontogiannis 等人的 [Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration](http://arxiv.org/pdf/2505.05262v1 ) 提出了一种新的状态建模框架和 MARL SMPE 算法，用于增强多智能体在部分可观测环境中的合作能力。**该方法通过推断非可观测状态的有意义表示，并结合对抗式探索策略，提高了智能体的策略判别能力和协作效率。** 实验表明，SMPE 在 MPE、LBF 和 RWARE 基准测试中优于现有算法。该成果于2025年发表于 ICML。

Luca Marzari 等人的 [Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation](http://arxiv.org/pdf/2505.05235v1 ) 提出了一种新的抽象 DNN 验证问题，通过层次化的不安全输出结构对深度神经网络进行更细粒度的安全性分析。**该方法利用抽象解释和输出可达集推理，能够在验证过程中评估多个安全级别，并减少计算开销。** 实验表明，该方法能够对对抗输入进行安全级别排序，提供更详细的安全性和鲁棒性评估。

Changxiang Wu 等人的 [Adaptive Biased User Scheduling for Heterogeneous Wireless Federate Learning Network](http://arxiv.org/pdf/2505.05231v1 ) 研究了在异构无线网络中高效部署联邦学习的方法，通过优化用户调度和资源分配来加速收敛。**该方法结合了收敛分析和深度强化学习，自适应地选择用户集并优化本地资源利用。** 实验结果表明，该框架在各种联邦学习任务中有效减少了任务时间。

Hendrik Surmann 等人的 [Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving](http://arxiv.org/pdf/2505.05223v1 ) 提出了一种基于多目标强化学习的自适应个性化自动驾驶方法。**该方法通过连续权重向量编码驾驶风格偏好，使智能体能够在运行时动态调整驾驶行为，而无需重新训练策略。** 实验表明，该智能体能够在复杂交通场景中根据变化的偏好调整驾驶风格，同时保持碰撞避免和路线完成的性能。

Min Dai 等人的 [Data-Driven Merton's Strategies via Policy Randomization](http://arxiv.org/pdf/2312.11797v2 ) 提出了一种在不完全市场中解决Merton期望效用最大化问题的方法。通过引入**策略随机化**，研究证明了其最优高斯策略的均值可以解决原始的Merton问题。该方法基于连续时间强化学习（RL）框架，设计了**在线和离线演员-评论家算法**，能够在无需估计模型参数的情况下实现数据驱动的策略学习。研究的一个重要洞见是，**RL和策略随机化不仅用于探索，还可以作为技术工具解决无法通过确定性策略解决的问题**。仿真和实证研究表明，所提出的RL算法在随机波动环境中显著优于传统的基于模型的插件方法。

Wang Xinyi 等人的 [Proxy Prompt: Endowing SAM and SAM 2 with Auto-Interactive-Prompt for Medical Segmentation](http://arxiv.org/pdf/2502.03501v3 ) 提出了一种名为**Proxy Prompt（PP）**的自动提示生成方法，旨在增强SAM和SAM2模型在医学分割中的自动化能力和人机交互。通过利用非目标数据和预标注掩码，PP能够自适应地选择最具代表性的上下文信息，并结合**视觉Mamba和选择性映射**提升分割性能。此外，研究还提出了**上下文着色模块**，通过双重反向交叉注意力增强目标特征与上下文嵌入之间的交互。实验表明，该方法在四个公共数据集上达到了**最先进的性能**，并且在仅使用16个图像掩码训练的情况下，表现与全训练模型相当。


> 📝 正在进行总结
### 主要研究方向

1. **机器人强化学习**
  - 研究方向概述：研究如何通过强化学习提升机器人在复杂环境中的自主性和适应性，包括人形机器人、腿式机器人等。
  - 代表性研究
    - *[Let Humanoids Hike! Integrative Skill Development on Complex Trails](http://arxiv.org/pdf/2505.06218v1 )* (2025年5月发表于 CVPR)
    - *[A Learning Framework for Diverse Legged Robot Locomotion Using Barrier-Based Style Rewards](http://arxiv.org/pdf/2409.15780v4 )* (2025年发表于 IEEE International Conference on Robotics and Automation (ICRA))

2. **多模态与多智能体强化学习**
  - 研究方向概述：探索强化学习在多模态数据和多智能体系统中的应用，包括多模态奖励模型、多智能体协作与优化等。
  - 代表性研究
    - *[R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](http://arxiv.org/pdf/2505.02835v2 )* (2025年5月发表于 arXiv)
    - *[Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration](http://arxiv.org/pdf/2505.05262v1 )* (2025年发表于 ICML)

3. **离线与数据高效的强化学习**
  - 研究方向概述：研究如何在离线数据集或有限数据条件下高效训练强化学习模型，包括离线多智能体强化学习和数据蒸馏。
  - 代表性研究
    - *[Offline Multi-agent Reinforcement Learning via Score Decomposition](http://arxiv.org/pdf/2505.05968v1 )* (2025年5月发表于 arXiv)
    - *[Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning](http://arxiv.org/pdf/2505.05701v1 )* (2025年5月发表于 arXiv)

4. **强化学习在通信与网络优化中的应用**
  - 研究方向概述：研究如何利用强化学习优化通信网络中的资源分配、信息更新和功率控制等任务。
  - 代表性研究
    - *[Efficient Information Updates in Compute-First Networking via Reinforcement Learning with Joint AoI and VoI](http://arxiv.org/pdf/2505.06025v1 )* (2025年5月发表于 arXiv)
    - *[Learning Power Control Protocol for In-Factory 6G Subnetworks](http://arxiv.org/pdf/2505.05967v1 )* (2025年发表于 IEEE EuCNC & 6G Summit)

5. **强化学习与生成模型的结合**
  - 研究方向概述：研究如何将强化学习与生成模型（如扩散模型、流匹配模型）结合，提升生成任务的质量和效率。
  - 代表性研究
    - *[Flow-GRPO: Training Flow Matching Models via Online RL](http://arxiv.org/pdf/2505.05470v1 )* (2025年5月发表于 arXiv)
    - *[Curriculum Direct Preference Optimization for Diffusion and Consistency Models](http://arxiv.org/pdf/2405.13637v6 )* (2025年发表于 CVPR)

6. **强化学习在安全与隐私保护中的应用**
  - 研究方向概述：研究如何通过强化学习在安全关键系统和隐私保护场景中实现安全约束和隐私保护。
  - 代表性研究
    - *[Barrier Function Overrides For Non-Convex Fixed Wing Flight Control and Self-Driving Cars](http://arxiv.org/pdf/2505.05548v1 )* (2025年5月发表于 arXiv)
    - *[Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled Systems via Particle Filter Reinforcement Learning](http://arxiv.org/pdf/2505.06122v1 )* (2025年发表于 L4DC)

### 研究趋势分析

过去几年，强化学习的研究趋势呈现出以下几个特点：

1. **多模态与多智能体强化学习的兴起**：随着多模态数据和多智能体系统的复杂性增加，强化学习在这些领域的应用逐渐成为研究热点，特别是在多模态奖励模型和多智能体协作优化方面。
2. **离线与数据高效强化学习的快速发展**：由于实际应用中数据获取成本高，离线强化学习和数据高效的强化学习方法受到越来越多的关注，研究者们致力于在有限数据条件下提升模型性能。
3. **生成模型与强化学习的深度融合**：生成模型（如扩散模型、流匹配模型）与强化学习的结合，为生成任务提供了新的解决方案，显著提升了生成质量和效率。
4. **安全与隐私保护的强化学习应用**：在安全关键系统和隐私保护场景中，强化学习的应用逐渐增多，研究者们通过屏障函数、隐私保护机制等方法，确保系统的安全性和隐私性。
5. **强化学习在通信与网络优化中的广泛应用**：随着5G、6G等通信技术的发展，强化学习在通信网络优化中的应用越来越广泛，特别是在资源分配、信息更新和功率控制等方面。

总体来看，强化学习的研究正朝着更加复杂、多模态、高效和安全的方向发展，未来有望在更多实际场景中得到广泛应用。
