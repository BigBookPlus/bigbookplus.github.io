Han Zhang 等人的 [Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting](http://arxiv.org/abs/2411.17674v2 ) 提出了一个名为 **Lantern** 的框架，**通过结合大语言模型（LLMs）和多模态特征来提升情感识别性能**。该框架利用多任务基础模型生成情感类别概率和维度分数，并通过LLMs（如GPT-4或Llama-3.1-405B）的外部知识调整预测结果。**关键创新**包括基于感受野的注意力加权模块，将对话切片处理以整合局部与全局信息。实验表明，在IEMOCAP数据集上，该框架将基线模型的准确率提升了1.23%（4分类）和1.80%（6分类）。

Han Wu 等人的 [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](http://arxiv.org/abs/2510.01677v1 ) 提出了一种**自适应门控融合网络（AGFN）**，用于解决多模态情感分析中模态质量不均（如噪声、缺失或冲突）的问题。该模型通过**双门机制**（基于信息熵和模态重要性）动态调整特征权重，抑制低质量模态的影响。在CMU-MOSI和CMU-MOSEI数据集上，AGFN显著优于基线模型，尤其在捕捉细微情感差异时表现鲁棒。可视化分析表明，AGFN通过降低特征位置与预测误差的相关性，生成了更泛化的多模态特征表示。

Jiye Lee 等人的 [Audio Driven Real-Time Facial Animation for Social Telepresence](http://arxiv.org/abs/2510.01176v1 ) 开发了一个**基于音频的实时3D面部动画系统**，用于虚拟现实中的社交临场感。**核心贡献**包括：1）扩散模型驱动的编码器，将音频转换为潜在面部表情序列；2）在线Transformer和单步去噪蒸馏技术，实现低延迟（<15ms GPU时间）。实验显示，该系统在动画准确性和速度上比现有离线方法提升100-1000倍，并支持多语言和多模态输入（如眼动追踪数据）。该成果将于SIGGRAPH Asia 2025发表。

Alexandrine Fortier 等人的 [Backdoor Attacks Against Speech Language Models](http://arxiv.org/abs/2510.01157v1 ) 首次系统研究了**语音语言模型的后门攻击**。实验覆盖四种语音编码器和三个数据集（涉及语音识别、情感识别等任务），攻击成功率高达90.76%-99.41%。**关键发现**包括：级联式多模态模型会继承所有组件的漏洞，且预训练编码器是最脆弱环节。作者还提出了一种基于微调的防御方法，可缓解中毒编码器的威胁。

Annemarie Hoffsommer 等人的 [DEAP DIVE: Dataset Investigation with Vision transformers for EEG evaluation](http://arxiv.org/abs/2510.00725v1 ) 探索了**低成本EEG设备的情感识别潜力**。通过连续小波变换将EEG信号转换为尺度图，并训练视觉Transformer（ViT）模型，仅用12个通道即可实现91.57%的四象限情感分类准确率（接近32通道的SOTA结果96.9%）。**创新点**在于证明了通道数量的大幅减少仍能保持高性能，为便携式EEG设备的应用提供了依据。该论文已被ICCV2025的ABAW Workshop接收。

Tianwen Zhou 等人的 [Editing Physiological Signals in Videos Using Latent Representations](http://arxiv.org/abs/2509.25348v2 ) 提出了一种基于潜在表示的视频生理信号编辑框架，用于解决面部视频中的心率（HR）隐私问题。**创新点**包括：通过预训练的3D变分自编码器（3D VAE）编码视频，并结合目标HR提示的文本嵌入，利用自适应层归一化（AdaLN）和特征级线性调制（FiLM）实现高保真视频重建与精确HR调制。实验表明，该方法在保持视觉质量（平均PSNR 38.96 dB，SSIM 0.98）的同时，HR调制误差仅为10.00 bpm MAE。该成果适用于生物信号匿名化或合成具有特定生命体征的视频。

Chetwin Low 等人的 [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](http://arxiv.org/abs/2510.01284v1 ) 提出了一种统一的双骨干跨模态融合范式Ovi，用于同步生成音频和视频。**核心创新**是通过块间跨模态融合（结合缩放RoPE嵌入和双向交叉注意力）实现音视频的自然同步，无需后处理对齐。模型通过并行训练视频和音频骨干（后者从零开始学习生成语音与音效），支持电影级视频片段生成，并在演讲者身份和情感传递上表现出色。

Balamurugan Thambiraja 等人的 [3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation](http://arxiv.org/abs/2509.26233v1 ) 提出了一种全卷积扩散模型3DiFACE，用于语音驱动的3D面部动画生成与编辑。**关键贡献**包括：支持单音频输入下多样化的唇部与头部运动生成，通过关键帧插值和风格个性化实现精细控制。该方法在保真度与多样性间取得平衡，并允许对现有动画进行局部修改。

Yuxuan Cai 等人的 [HumanVideo-MME: Benchmarking MLLMs for Human-Centric Video Understanding](http://arxiv.org/abs/2507.04909v2 ) 提出了HV-MMBench基准，全面评估多模态大模型（MLLMs）在人类中心视频理解中的能力。**特色**包括：覆盖13项任务（如年龄估计、情感识别、意图预测）、多题型设计（选择题/填空题等）、50种视觉场景和10秒至30分钟的视频时长跨度。该基准支持对模型感知与认知能力的系统评估。

Siddhant Sukhana 等人的 [FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos](http://arxiv.org/abs/2509.25745v1 ) 研究了多模态大模型在金融短视频主题标注中的应用。**发现**表明：仅视频模态在情感分析等任务中表现突出（依赖表情/肢体语言线索），而多模态组合（如TV或AV）可能因噪声干扰效果反而不如单模态。该研究为ICCV短视频理解研讨会论文。

Jiacheng Shi 等人的 [Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition](http://arxiv.org/abs/2509.25458v1 ) 提出了 **CCoT-Emo** 框架，通过结构化情感图（Emotion Graphs, EGs）增强大型音频-语言模型（LALMs）在零样本语音情感识别（SER）中的推理能力。**EGs整合了七种声学特征（如基频、语速）、文本情感和跨模态关联**，以可解释的组成式表示指导模型推理。实验表明，该方法在多个SER基准上优于现有零样本基线，并显著提升推理准确性。

Yuntao Shou 等人的 [Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](http://arxiv.org/abs/2509.24322v1 ) 对多模态大语言模型（MLLMs）在情感识别与推理领域的应用进行了系统性综述，涵盖模型架构、数据集和性能基准。**该研究首次整合了MLLMs在跨模态情感分析中的进展**，并指出未来研究方向，如更高层次的语义建模和复杂场景下的跨模态融合。论文未提及具体发表会议/期刊。

Junjie Li 等人的 [Xi+: Uncertainty Supervision for Robust Speaker Embedding](http://arxiv.org/abs/2509.05993v3 ) 提出了一种改进的说话人嵌入模型 **xi+**，通过**时空注意力模块**和新型损失函数 **Stochastic Variance Loss** 显式监督不确定性学习。**该方法在VoxCeleb1-O和NIST SRE 2024数据集上分别实现10%和11%的性能提升**，显著增强了模型对情感、语言等干扰因素的鲁棒性。

Wenyu Zhang 等人的 [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](http://arxiv.org/abs/2506.06820v2 ) 探索了基于生成式推理的语音情感识别方法，通过**多任务学习框架**使AudioLLMs产生证据支撑的情感解释。**在IEMOCAP和MELD数据集上，该方法不仅提高了预测准确率，还增强了生成响应的逻辑连贯性**，并在跨领域数据中展现出泛化能力。

Muyun Jiang 等人的 [ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and Querying](http://arxiv.org/abs/2509.24302v1 ) 提出了一种EEG-语言对齐的基础模型 **ELASTIQ**，通过**任务感知的语义引导**生成结构化EEG嵌入。**模型在20个跨任务数据集中14个达到SOTA性能**，首次揭示了语言指令作为语义先验对EEG表征学习的引导作用。其核心创新包括联合频谱-时序重建模块和指令条件化查询转换器（IQF）。

Bo-Hao Su 等人的 [Reasoning Beyond Majority Vote: An Explainable SpeechLM Framework for Speech Emotion Recognition](http://arxiv.org/abs/2509.24187v1 ) 提出了一个可解释的语音语言模型（SpeechLM）框架，将语音情感识别（SER）任务转化为生成式推理任务。**创新点**包括：模型首先生成语音转录文本，随后输出情感标签及基于词汇和声学线索的自然语言解释。通过教师大语言模型生成的解释作为中间监督信号，结合多数投票标签进行微调。该方法在保持分类性能的同时提升了可解释性，并通过支持任意标注者标签匹配的评估指标验证了有效性。实验表明，模型生成的解释被人类评估者认为合理且具有 grounding 性。

Haoyu Song 等人的 [An Efficient Transfer Learning Method Based on Adapter with Local Attributes for Speech Emotion Recognition](http://arxiv.org/abs/2509.23795v1 ) 提出了一种基于适配器的高效迁移学习方法，解决语音情感识别中高质量标注数据稀缺的问题。**核心贡献**包括：1）设计轻量级加权平均池化-Transformer（WAP-Transformer）框架增强帧级表示；2）采用教师-学生分支的适配器结构，通过掩码预测和自蒸馏目标联合优化学生分支，教师分支通过指数移动平均更新；3）引入无监督聚类学习局部属性作为通用语义监督。统计注意力池化（SAP）模块用于微调阶段的语句表示生成。在IEMOCAP数据集上取得了优于同类方法的性能。

Yudong Yang 等人的 [Audio-centric Video Understanding Benchmark without Text Shortcut](http://arxiv.org/abs/2503.19951v3 ) 提出了以音频为中心的视频理解基准（AVUT），评估多模态大模型对视频中听觉信息的理解能力。**关键创新**包括：1）设计避免文本捷径问题的答案排列过滤机制；2）全面测试音频内容及音视频交互理解任务。该成果于2025年9月发表于 EMNLP 2025。

Taewon Kang 等人的 [Action2Dialogue: Generating Character-Centric Narratives from Scene-Level Prompts](http://arxiv.org/abs/2505.16819v3 ) 提出从场景级提示生成角色驱动的叙事对话，通过递归叙事库（Recursive Narrative Bank）实现角色对话的上下文和情感一致性。**技术亮点**包括：结合视觉语言编码器提取语义特征，并利用大语言模型合成自然对话，最终生成带情感表达的多模态视频叙事。

Anupam Purwar 等人的 [i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents](http://arxiv.org/abs/2509.20971v2 ) 分析了低延迟端到端语音对话系统的优化方法，**核心发现**表明：文本转语音（TTS）组件对实时性影响最大，通过减少残差向量量化（RVQ）迭代次数可有效降低延迟。该成果于2025年9月发表于 AIML Systems 2025。

Zheng Lian 等人的 [EmoPrefer: Can Large Language Models Understand Human Emotion Preferences?](http://arxiv.org/abs/2507.04278v4 ) 提出了 **首个探索LLM理解人类情感偏好的研究框架EmoPrefer**，包含高质量专家标注的情感偏好数据集EmoPrefer-Data和评估基准EmoPrefer-Bench。该研究揭示了 **多模态LLM在情感偏好预测中的潜力与局限性**，为描述性多模态情感识别（DMER）领域提供了新方向，并推动了更智能的人机交互发展。

Daiqing Wu 等人的 [Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach](http://arxiv.org/abs/2509.21950v1 ) 提出了一种 **开放式视觉情感评估框架**，通过情感陈述判断任务（Emotion Statement Judgment）解决现有评估方法的局限性（如忽略合理响应、有限情感分类等）。研究发现 **MLLMs在情感解释和基于上下文的判断中表现较强**，但在感知主观性理解方面仍存在显著差距，与人类表现相比有改进空间。

Qing Zhu 等人的 [Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition](http://arxiv.org/abs/2509.21747v1 ) 提出了一种 **融合视觉场景上下文和标签语义信息的群体情感识别框架**。通过多尺度场景编码和情感树结构化的标签语义增强，该方法在三个主流GER数据集上达到SOTA性能。该成果已投稿至IEEE Transactions on Human-Machine Systems。

Jing-Tong Tzeng 等人的 [Lessons Learnt: Revisit Key Training Strategies for Effective Speech Emotion Recognition in the Wild](http://arxiv.org/abs/2508.07282v2 ) 通过 **重新审视训练策略**（如平衡采样、激活函数优化和微调技术）提升自然场景下的语音情感识别性能。研究发现 **单独微调单模态特征提取器后融合特征** 能取得最佳效价预测效果（CCC=0.6953），该成果发表于Interspeech 2025。

Nicola Fabiano 的 [Affective Computing and Emotional Data: Challenges and Implications in Privacy Regulations, The AI Act, and Ethics in Large Language Models](http://arxiv.org/abs/2509.20153v2 ) 探讨了 **情感数据在GDPR和欧盟AI法案下的合规挑战**，指出情感数据可能被视为敏感个人数据，需强化目的限制、数据最小化和知情同意机制。研究强调需平衡AI情感识别能力与伦理隐私保护。

Sadia Abdulhalim 等人的 [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](http://arxiv.org/abs/2509.22729v1 ) 提出 **动态注意力融合框架（DAF）**，通过自适应权重整合文本与语音特征。在未微调基础编码器的情况下，DAF显著超越单模态和静态融合基线，尤其擅长建模情感复杂输入。该成果将被发表于ACS/IEEE AICCSA 2023会议。

以下是关于情绪识别领域最新论文的中文摘要整理：

1. Hsiao-Ying Huang 等人的 [MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model](http://arxiv.org/abs/2509.20706v1 ) 提出了**基于互信息去噪的标签融合框架MI-Fuse**，用于解决语音情绪识别（SER）中的域适应问题。该研究创新性地结合闭源大型音频语言模型（LALM）和源域训练的SER分类器作为双教师模型，通过互信息加权和指数移动平均策略提升目标域性能。实验表明，该方法在六种跨域迁移场景中平均超越基线3.9%，且**无需共享源域数据**。

2. Jialong Mai 等人的 [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](http://arxiv.org/abs/2509.18196v2 ) 发布了目前**最全面的中文非语言声音（NV）数据集MNV-17**，包含17类清晰标注的情感性非语言发声（如叹息、笑声）。该数据集通过表演式录制保证高质量样本，填补了现有ASR系统在情感表达理解上的空白。研究测试了四种主流ASR架构的联合语义转录与NV分类性能，为情感语音识别提供新基准。该成果将发表于ICASSP 2026。

3. Phyo Thet Yee 等人的 [SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](http://arxiv.org/abs/2509.19965v1 ) 开发了**多模态情感嵌入的说话人脸生成框架**，整合文本情感分析与音频价态-唤醒特征来增强表情真实性。创新点包括音频-运动对齐模块和LLM生成的场景描述条件输入，在运动多样性和视频流畅度上显著优于现有方法。该成果被WACV 2026接收。

4. Sarmistha Das 等人的 [When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset](http://arxiv.org/abs/2509.19952v1 ) 提出了**视频投诉描述生成新任务（CoD-V）**，并发布包含1,175个情感标注投诉视频的ComVID数据集。研究设计了投诉保留度（CR）评估指标，开发了融合RAG的VideoLLaMA2-7b模型，能结合用户情绪状态生成结构化投诉文本。

5. Jiahao Tang 等人的 [SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition](http://arxiv.org/abs/2507.17524v2 ) 针对脑电情绪识别提出**语义-动态一致性域适应框架**，包含：同被试同试验混合数据增强、RKHS空间动态分布对齐模块和双域相似性一致性学习机制。在SEED等三个基准上实现跨被试/跨会话state-of-the-art性能，为个性化情感BCI奠定基础。

Shao-Yu Chang 等人的 [Talking Head Generation via AU-Guided Landmark Prediction](http://arxiv.org/abs/2509.19749v1 ) 提出了一种基于面部动作单元（AU）的音频驱动说话头生成框架。**该方法通过将AU显式映射到2D面部关键点，实现了精确的表情控制**。其两阶段架构（运动生成+扩散合成）在MEAD数据集上表现优异，提升了表情准确性、时间连贯性和视觉真实感。

Mohammad Saim 等人的 [Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models](http://arxiv.org/abs/2509.19595v1 ) 利用大视觉语言模型（LVLM）生成具身情感叙事（ELENA）。**研究发现模型存在面部区域偏见，但在面部遮挡情况下仍能有效识别身体姿态情绪**。该方法为跨模态情感分析提供了新思路。

Jiajun He 等人的 [M4SER: Multimodal, Multirepresentation, Multitask, and Multistrategy Learning for Speech Emotion Recognition](http://arxiv.org/abs/2509.18706v1 ) 提出多模态语音情感识别方法。**通过对抗网络增强模态特异性表征，并结合标签对比学习优化情感特征提取**。在IEMOCAP和MELD数据集上验证了其优越性。该成果于2024年9月发表于 IEEE Transactions on Audio, Speech and Language Processing。

### 主要研究方向 📝

1. **多模态情感识别与推理**
  - 研究方向概述：结合视觉、语音、文本等多模态信息进行情感识别，并探索大语言模型（LLMs）在多模态情感推理中的应用。
  - 代表性研究
    - *[Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting](http://arxiv.org/abs/2411.17674v2 )* (2024年11月发表于arXiv)
    - *[Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](http://arxiv.org/abs/2510.01677v1 )* (2025年10月发表于arXiv)
    - *[Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](http://arxiv.org/abs/2509.24322v1 )* (2025年9月发表于arXiv)

2. **语音情感识别（SER）与语音驱动面部动画**
  - 研究方向概述：通过语音信号识别情感，并生成相应的面部动画或情感表达。
  - 代表性研究
    - *[Audio Driven Real-Time Facial Animation for Social Telepresence](http://arxiv.org/abs/2510.01176v1 )* (2025年10月发表于SIGGRAPH Asia 2025)
    - *[Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition](http://arxiv.org/abs/2509.25458v1 )* (2025年9月发表于arXiv)
    - *[An Efficient Transfer Learning Method Based on Adapter with Local Attributes for Speech Emotion Recognition](http://arxiv.org/abs/2509.23795v1 )* (2025年9月发表于arXiv)

3. **脑电（EEG）情感识别**
  - 研究方向概述：利用脑电信号进行情感识别，探索低成本EEG设备的应用潜力。
  - 代表性研究
    - *[DEAP DIVE: Dataset Investigation with Vision transformers for EEG evaluation](http://arxiv.org/abs/2510.00725v1 )* (2025年10月发表于ICCV2025 ABAW Workshop)
    - *[ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and Querying](http://arxiv.org/abs/2509.24302v1 )* (2025年9月发表于arXiv)
    - *[SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition](http://arxiv.org/abs/2507.17524v2 )* (2025年7月发表于arXiv)

4. **情感数据隐私与伦理**
  - 研究方向概述：研究情感数据在隐私保护、合规性及伦理方面的挑战与解决方案。
  - 代表性研究
    - *[Affective Computing and Emotional Data: Challenges and Implications in Privacy Regulations, The AI Act, and Ethics in Large Language Models](http://arxiv.org/abs/2509.20153v2 )* (2025年9月发表于arXiv)
    - *[Editing Physiological Signals in Videos Using Latent Representations](http://arxiv.org/abs/2509.25348v2 )* (2025年9月发表于arXiv)

5. **视觉情感识别与生成**
  - 研究方向概述：通过视觉信息（如面部表情、身体姿态）识别情感，并生成相应的情感表达内容。
  - 代表性研究
    - *[3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation](http://arxiv.org/abs/2509.26233v1 )* (2025年9月发表于arXiv)
    - *[SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](http://arxiv.org/abs/2509.19965v1 )* (2025年9月发表于WACV 2026)
    - *[Talking Head Generation via AU-Guided Landmark Prediction](http://arxiv.org/abs/2509.19749v1 )* (2025年9月发表于arXiv)

6. **情感偏好与用户反馈建模**
  - 研究方向概述：研究用户情感偏好及其在个性化系统中的应用。
  - 代表性研究
    - *[EmoPrefer: Can Large Language Models Understand Human Emotion Preferences?](http://arxiv.org/abs/2507.04278v4 )* (2025年7月发表于arXiv)
    - *[When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset](http://arxiv.org/abs/2509.19952v1 )* (2025年9月发表于arXiv)

### 研究趋势分析

过去几年，情感识别领域的研究呈现以下趋势：  
1. **多模态融合成为主流**：从单一模态（如语音或文本）转向多模态（语音、视觉、文本、EEG等）协同建模，尤其关注大语言模型（LLMs）在多模态情感推理中的潜力。  
2. **零样本与迁移学习兴起**：研究者探索零样本学习和域适应技术，以解决标注数据稀缺和跨域泛化问题，如语音情感识别中的闭源模型适配。  
3. **可解释性与生成式推理**：情感识别模型逐渐从分类任务转向生成式推理，注重可解释性（如生成情感解释文本）和动态交互能力。  
4. **隐私与伦理受重视**：随着GDPR和AI法案的实施，情感数据的隐私保护、合规性及伦理问题成为研究热点。  
5. **低成本与轻量化**：脑电（EEG）等生理信号的情感识别研究强调减少设备依赖，推动便携式应用的落地。  
6. **情感生成技术发展**：语音/视觉驱动的面部动画技术更加精准，结合情感嵌入实现更自然的人机交互。